# -*- coding: utf-8 -*-
"""DL gender identification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RE87svuP3a2w06KqrUF2b7o65Hfa642r
"""

from google.colab import drive
drive.mount('/content/drive')

import glob
import librosa
import librosa.display
import matplotlib.pyplot as plt
import numpy as np

"""Reading wave file"""

wave_file = "/content/drive/MyDrive/dataset1/f0001_us_f0001_00001.wav"

signal,sample_rate = librosa.load(wave_file)

signal

sample_rate

"""Plotting"""

plt.figure(figsize = (14, 5))
librosa.display.waveshow(signal, sr = 1000)

"""Reading Multiple wave files from folder"""

myfolder = "/content/drive/MyDrive/dataset1"
myfolder

def getwavefile(path):
  wave_files = glob.glob(path + "/*.wav")
  plt.figure(figsize = (14, 5))
  for i  in wave_files:
    signal,sample_rate = librosa.load(i)
    librosa.display.waveshow(signal, sr = 1000)

# getwavefile(myfolder)

"""DataFrame"""

import pandas as pd
import glob

myfolder = "/content/drive/MyDrive/dataset1"

def getwavfile(path):
  wave_file = glob.glob(path +"/*.wav")
  return wave_file

data = getwavfile(myfolder)

data

wave_file = glob.glob(myfolder +"/*.wav")
names=[]
label_list=[]
for h in wave_file:
  k=h.split("1/")
  names.append(k[1])
  print(k[1][0])
  if k[1][0]=='f':
    label_list.append("Female")
  else:
    label_list.append("male")

names

label_list

import pandas as pd

df1 = pd.DataFrame(names, columns = ["names"])
df1

df2 = pd.DataFrame(label_list, columns = ["label_list"])
df2

df3 = pd.DataFrame(data, columns = ["wave_file"])
df3

df=pd.concat([df1,df2,df3],axis=1)
df

"""feature extraction:"""

df5 = pd.DataFrame(signal, columns = ['Signals'])
df5

"""Feature Extraction:"""

import scipy
_path = "/content/drive/MyDrive/dataset1"

meanlist=[]
stdlist=[]
maxvlist=[]
type_list= []
n_list= []

"""jitter, shimmer, hnr

"""

pip install praat-parselmouth

import glob
import parselmouth
import scipy
import os
from sklearn.model_selection import train_test_split

wavefiles=glob.glob(_path+"/*.wav")
j_list = []
s_list = []
h_list = []
type_list = []

for wav_file in  wavefiles :
  sound = parselmouth.Sound(wav_file) # sound object from wav file
  pitch = sound.to_pitch()
  pulses = parselmouth.praat.call([sound, pitch], "To PointProcess (cc)")

        # name analysis

  name = os.path.basename(wav_file).split(".")[0]

       # jitter
  jitter_local = parselmouth.praat.call(pulses, "Get jitter (local)", 0.0, 0.0, 0.0001, 0.02, 1.3) * 100

        # shimmer
  shimmer_local = parselmouth.praat.call([sound, pulses], "Get shimmer (local)", 0, 0, 0.0001, 0.02, 1.3, 1.6)

        # HNR
  harmonicity = parselmouth.praat.call(sound, "To Harmonicity (cc)", 0.01, 75, 0.1, 1.0)
  hnr = parselmouth.praat.call(harmonicity, "Get mean", 0, 0)

        # Append to numpy array

  j_list.append(jitter_local)
  s_list.append(shimmer_local)
  h_list.append(hnr)

  if jitter_local >2.10:
    type_list.append("1")
  else:
    type_list.append("0")

  x, sr = librosa.load(wav_file)
  freqs = np.fft.fftfreq(x.size)

import pandas as pd
def get_voice_data(_path):




  return j_list, s_list, h_list

j_list, s_list, h_list   = get_voice_data(_path)

cols=["j_list", "s_list", "h_list" ]

jlist = pd.DataFrame(j_list, columns = ['Jitter'])
slist = pd.DataFrame(s_list, columns = ['Shimmer'])
hlist = pd.DataFrame(h_list, columns = ['HNR'])


Feature2 = pd.concat([ jlist,slist,hlist,df2], axis = 1)

df2

import pandas as pd
import matplotlib.pyplot as plt

# Assuming you have already created the DataFrames jlist, slist, and hlist

# Plotting 'Jitter'
plt.figure(figsize=(8, 6))
plt.plot(jlist['Jitter'], label='Jitter')
plt.xlabel('Sample Index')
plt.ylabel('Jitter Value')
plt.title('Jitter Plot')
plt.legend()
plt.show()

# Plotting 'Shimmer'
plt.figure(figsize=(8, 6))
plt.plot(slist['Shimmer'], label='Shimmer')
plt.xlabel('Sample Index')
plt.ylabel('Shimmer Value')
plt.title('Shimmer Plot')
plt.legend()
plt.show()

# Plotting 'HNR'
plt.figure(figsize=(8, 6))
plt.plot(hlist['HNR'], label='HNR')
plt.xlabel('Sample Index')
plt.ylabel('HNR Value')
plt.title('HNR Plot')
plt.legend()
plt.show()

for column in df2.columns:
    plt.hist(df2[column])
    plt.show()

Feature2

"""DataFrame to CSV file"""

# csv_data = Feature.to_csv(header = True)
# Feature.to_csv("/content/drive/MyDrive/feature.csv")

csvFile = pd.read_csv('/content/drive/MyDrive/feature.csv')
metadata=pd.DataFrame(csvFile)

df2

df3 = pd.DataFrame(data, columns = ["wave_file"])
df3

"""Chroma features"""

#Chroma feature
import pandas as pd
import librosa
import numpy as np
# df3 = pd.DataFrame() # Assuming you have a dataframe 'df3' containing audio file paths
scaled_feature1 = []
signal = []
if 'wave_file' in df3.columns:
    for i in df3['wave_file']:
      s, sample_rate = librosa.load(i, sr=8000)
      chroma=librosa.feature.chroma_stft(y=s, sr=8000)
      chroma_feature = np.mean(chroma.T, axis=0)
      scaled_feature1.append(chroma_feature)
# df3['scaled_feature'] = scaled_feature
else:
    print("Column 'wave_file' not found in the dataframe.")

scaled_feature1

chr=pd.DataFrame(scaled_feature1)
chr

feat= pd.concat([chr,df2], axis = 1)
feat

"""MODEL BUILDING OF chroma feature USING RNN"""

from sklearn.preprocessing import LabelEncoder

# Extract features and labels
features = feat.iloc[:,:-1]
labels = feat.label_list

# Convert labels to integers using LabelEncoder
label_encoder = LabelEncoder()
labels = label_encoder.fit_transform(labels)

# Split the data into training and testing sets
x_train11, x_test11, y_train11, y_test11 = train_test_split(features, labels, test_size=0.5, random_state=42)

# Normalize the features to range [0, 1]
x_train11 = x_train11.astype('float32') / np.max(features)
x_test11 = x_test11.astype('float32') / np.max(features)

x_train11 = x_train11.values
x_test11 = x_test11.values

x_train11 = x_train11.reshape(x_train11.shape[0], x_train11.shape[1], 1)
x_test11 = x_test11.reshape(x_test11.shape[0], x_test11.shape[1], 1)

num_classes = len(np.unique(labels))
y_train11 = keras.utils.to_categorical(y_train11, num_classes)
y_test11 = keras.utils.to_categorical(y_test11, num_classes)

model11 = keras.Sequential([
    layers.LSTM(64, input_shape=(x_train11.shape[1], 1), return_sequences=True),
    layers.LSTM(64),
    layers.Dense(128, activation='relu'),
    layers.Dense(num_classes, activation='softmax')
])

model11.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
batch_size = 16
epochs = 20

model11.fit(x_train11, y_train11, batch_size=batch_size, epochs=epochs, validation_split=0.1)

loss, accuracy = model11.evaluate(x_test11, y_test11)
print('Test accuracy:', accuracy)

"""chroma feature sequential"""

X10= feat.iloc[:,:-1]
y10= feat.label_list

x_train10, x_test10, y_train10,  y_test10 = train_test_split(X10,y10, test_size=0.5, random_state=42)

model10 = Sequential()
model10.add(Dense(64, activation='relu', input_dim=x_train10.shape[1]))
model10.add(Dense(64, activation='relu'))
model10.add(Dense(1, activation='sigmoid'))

from sklearn.preprocessing import LabelEncoder

# Instantiate the LabelEncoder
label_encoder = LabelEncoder()

# Convert string labels to numeric values
y_train10 = label_encoder.fit_transform(y_train10)

model10.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

model10.fit(x_train10, y_train10, epochs=10, batch_size=32)

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
y_test_encoded = label_encoder.fit_transform(y_test10)

loss, accuracy = model10.evaluate(x_test10, y_test_encoded)
print("Test Loss:", loss)
print("Test Accuracy:", accuracy)

"""MODEL BUILDING OF JITTER SHIMMER USING RNN"""

from sklearn.preprocessing import LabelEncoder

# Extract features and labels
features = Feature2.iloc[:,:-1]
labels = Feature2.label_list

# Convert labels to integers using LabelEncoder
label_encoder = LabelEncoder()
labels = label_encoder.fit_transform(labels)

# Split the data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.5, random_state=42)

# Normalize the features to range [0, 1]
x_train = x_train.astype('float32') / np.max(features)
x_test = x_test.astype('float32') / np.max(features)

# Reshape the data to 3D format (num_samples, num_timesteps, num_features)
# x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)
# x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)

# # Convert labels to one-hot encoding
# num_classes = len(np.unique(labels))
# y_train = keras.utils.to_categorical(y_train, num_classes)
# y_test = keras.utils.to_categorical(y_test, num_classes)

x_train = x_train.values
x_test = x_test.values
x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)
x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)
num_classes = len(np.unique(labels))
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)
model = keras.Sequential([
    layers.LSTM(64, input_shape=(x_train.shape[1], 1), return_sequences=True),
    layers.LSTM(64),
    layers.Dense(128, activation='relu'),
    layers.Dense(num_classes, activation='softmax')
])
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
batch_size = 16
epochs = 20

model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)
loss, accuracy = model.evaluate(x_test, y_test)
print('Test accuracy:', accuracy)

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow import keras
from tensorflow.keras import layers

loss, accuracy = model.evaluate(x_test, y_test)
print('Test accuracy:', accuracy)

# Specify the path where you want to save the model
model_path = '/content/drive/MyDrive/Rnn_saved_model.h5'

# Save the model to the specified path
model.save(model_path)

print("Model saved successfully at:", model_path)



"""Model Building OF JITTER SHIMMER USING SEQUENTIAL"""

X9= Feature2.iloc[:,:-1]
y9= Feature2.label_list

X9

x_train9, x_test9, y_train9,  y_test9 = train_test_split(X9,y9, test_size=0.1, random_state=42)

wave_file = glob.glob(myfolder +"/*.wav")

pip install tensorflow

pip install keras

import keras
import tensorflow as tf
from tensorflow.keras.models import Model
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from keras.layers import Embedding

model2 = Sequential()
model2.add(Dense(64, activation='relu', input_dim=x_train9.shape[1]))
model2.add(Dense(64, activation='relu'))
model2.add(Dense(1, activation='sigmoid'))

from sklearn.preprocessing import LabelEncoder

# Instantiate the LabelEncoder
label_encoder = LabelEncoder()

# Convert string labels to numeric values
y_train9 = label_encoder.fit_transform(y_train9)

model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

model2.fit(x_train9, y_train9, epochs=10, batch_size=32)

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
y_test_encoded = label_encoder.fit_transform(y_test9)

loss, accuracy = model2.evaluate(x_test9, y_test_encoded)
print("Test Loss:", loss)
print("Test Accuracy:", accuracy)

"""**MFCC**"""

from tensorflow.keras.models import Model

import pandas as pd
import librosa
import numpy as np

# df3 = pd.DataFrame()  # Assuming you have a dataframe 'df3' containing audio file paths

scaled_feature = []
signal = []

if 'wave_file' in df3.columns:
    for i in df3['wave_file']:
        s, sample_rate = librosa.load(i, sr=22050)
        g = librosa.feature.mfcc(y=s, sr=sample_rate, n_mfcc=12)
        mfcc_scaled = np.mean(g.T, axis=0)
        scaled_feature.append(mfcc_scaled)

    df3['scaled_feature'] = scaled_feature
else:
    print("Column 'wave_file' not found in the dataframe.")

scaled_feature

new1 = pd.DataFrame(scaled_feature, columns = ["0","1","2","3","4","5","6","7","8","9","10","11",])

new1

mfcc_meta = pd.concat([new1,df2], axis = 1)

mfcc_meta

""" Sequential Model building of **MFCC**"""

X1 = mfcc_meta.iloc[:,:-1]
y1 = mfcc_meta.label_list

X1

X1.shape

y1.shape

X_train1,x_test1,y_train1,y_test1 = train_test_split(X1,y1,test_size=0.5,random_state = 1)

X_train1.shape

x_test1.shape

model = Sequential()
model.add(Dense(64, activation='relu', input_dim=X_train1.shape[1]))
model.add(Dense(64, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

from sklearn.preprocessing import LabelEncoder

# Instantiate the LabelEncoder
label_encoder = LabelEncoder()

# Convert string labels to numeric values
y_train1 = label_encoder.fit_transform(y_train1)

X_train1.shape

y_train1.shape

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(X_train1, y_train1, epochs=10, batch_size=32)

label_encoder = LabelEncoder()
y_test_encoded = label_encoder.fit_transform(y_test1)

loss, accuracy = model.evaluate(x_test1, y_test_encoded)
print("Test Loss:", loss)
print("Test Accuracy:", accuracy)

"""RNN Model building of MFCC"""

# Extract features and labels
features = mfcc_meta.iloc[:,:-1]
labels = mfcc_meta.label_list

# Convert labels to integers using LabelEncoder
label_encoder = LabelEncoder()
labels = label_encoder.fit_transform(labels)

# Split the data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.5, random_state=42)

# Normalize the features to range [0, 1]
x_train = x_train.astype('float32') / np.max(features)
x_test = x_test.astype('float32') / np.max(features)

# Reshape the data to 3D format (num_samples, num_timesteps, num_features)
# x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)
# x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)

# # Convert labels to one-hot encoding
# num_classes = len(np.unique(labels))
# y_train = keras.utils.to_categorical(y_train, num_classes)
# y_test = keras.utils.to_categorical(y_test, num_classes)

num_classes = len(np.unique(labels))
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)
model = keras.Sequential([
    layers.LSTM(64, input_shape=(x_train.shape[1], 1), return_sequences=True),
    layers.LSTM(64),
    layers.Dense(128, activation='relu'),
    layers.Dense(num_classes, activation='softmax')
])
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
batch_size = 16
epochs = 20

model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)
loss, accuracy = model.evaluate(x_test, y_test)
print('Test accuracy:', accuracy)

df3 = pd.DataFrame(data, columns = ["wave_file"])
df3

"""

Using SVM (dataset 1)"""

X3 = Feature2.iloc[:,:-1]
y3 = Feature2.label_list

X3

y3

X_train1,x_test1,y_train1,y_test1 = train_test_split(X3,y3,test_size=0.2,random_state = 1)

X_train1.shape

def score(model):
    print("Training score: ",model.score(X_train1,y_train1))
    print("Test score: ",model.score(x_test1,y_test1))

from sklearn.svm import SVC

SVMmodel = SVC(kernel = 'rbf', C=2.0,random_state=0,degree = 3)
SVMmodel.fit(X_train1,y_train1)
ypred1 = SVMmodel.predict(x_test1)
score(SVMmodel)

"""Using Decision Tree"""

from sklearn.tree import DecisionTreeClassifier

DTmodel = DecisionTreeClassifier(min_samples_split = 5,max_depth = 10,random_state = 0)

DTmodel.fit(X_train1,y_train1)

ypred2 = DTmodel.predict(x_test1)
ypred2[:5]

score(DTmodel)

"""LogisticRegression¶"""

from sklearn.linear_model import LogisticRegression

from sklearn.linear_model import LogisticRegression
LRmodel = LogisticRegression(n_jobs=3,max_iter=1000,random_state=0)

LRmodel.fit(X_train1,y_train1)

ypred3 = LRmodel.predict(x_test1)

score(LRmodel)

"""Using K-Nearest Neighbors"""

from sklearn.neighbors import KNeighborsClassifier

from sklearn.neighbors import KNeighborsClassifier
Kmodel = KNeighborsClassifier(n_neighbors = 50,metric ='minkowski',p=1,n_jobs=5,algorithm='ball_tree')
Kmodel.fit(X_train1,y_train1)
ypred4 = Kmodel.predict(x_test1)
score(Kmodel)

Kmodel.fit(X_train1,y_train1)

ypred4 = Kmodel.predict(x_test1)

score(Kmodel)

"""gender classification ensemble Soft voting"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import VotingClassifier
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier


# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# # Define the individual classifiers
# clf1 = DecisionTreeClassifier()
SVMmodel = SVC(probability=True)
# clf3 = KNeighborsClassifier()

# Create the ensemble model using VotingClassifier
ensemble_model = VotingClassifier(estimators=[('svm', SVMmodel),('dt', DTmodel), ('lr', LRmodel), ('knn', Kmodel)], voting='soft')

# Train the ensemble model
ensemble_model.fit(X_train1, y_train1)

# Make predictions on the testing set
y_pred = ensemble_model.predict(x_test1)

# Calculate the accuracy of the ensemble model
accuracy = accuracy_score(y_test1, y_pred)
print("Accuracy:", accuracy)

"""gender classification ensemble Hard voting"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import VotingClassifier
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier


# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# # Define the individual classifiers
# clf1 = DecisionTreeClassifier()
SVMmodel = SVC(probability=True)
# clf3 = KNeighborsClassifier()

# Create the ensemble model using VotingClassifier
ensemble_model = VotingClassifier(estimators=[('svm', SVMmodel),('dt', DTmodel), ('lr', LRmodel), ('knn', Kmodel)], voting='hard')

# Train the ensemble model
ensemble_model.fit(X_train1, y_train1)

# Make predictions on the testing set
y_pred = ensemble_model.predict(x_test1)

# Calculate the accuracy of the ensemble model
accuracy = accuracy_score(y_test1, y_pred)
print("Accuracy:", accuracy)

csvFile1 = pd.read_csv('/content/drive/MyDrive/dataset csv/voice.csv')
metadata1=pd.DataFrame(csvFile1)

metadata1

"""SVM (dataset 2)"""

X4 = metadata1.iloc[:,:-1]
y4 = metadata1.label

y4

from sklearn.model_selection import train_test_split

X_train5,x_test5,y_train5,y_test5 = train_test_split(X4,y4,test_size=0.2,random_state = 1)

def score(model):
    print("Training score: ",model.score(X_train5,y_train5))
    print("Test score: ",model.score(x_test5,y_test5))

from sklearn.svm import SVC

SVMmodel = SVC(kernel = 'rbf', C=2.0,random_state=0,degree = 3)
SVMmodel.fit(X_train5,y_train5)
ypred5 = SVMmodel.predict(x_test5)
score(SVMmodel)

"""DT"""

from sklearn.tree import DecisionTreeClassifier

DTmodel3 = DecisionTreeClassifier(min_samples_split = 5,max_depth = 10,random_state = 0)

DTmodel3.fit(X_train5,y_train5)

ypred6 = DTmodel3.predict(x_test5)
ypred6[:5]

score(DTmodel3)

"""LR"""

from sklearn.linear_model import LogisticRegression
LRmodel = LogisticRegression(n_jobs=3,max_iter=1000,random_state=0)

LRmodel.fit(X_train5,y_train5)
ypred7 = LRmodel.predict(x_test5)
score(LRmodel)

"""KNN"""

from sklearn.neighbors import KNeighborsClassifier
Kmodel = KNeighborsClassifier(n_neighbors = 50,metric ='minkowski',p=1,n_jobs=5,algorithm='ball_tree')
Kmodel.fit(X_train5,y_train5)
ypred8 = Kmodel.predict(x_test5)
score(Kmodel)

"""ENSEMBLE soft voting"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import VotingClassifier
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier



SVMmodel = SVC(probability=True)


# Create the ensemble model using VotingClassifier
ensemble_model = VotingClassifier(estimators=[('svm', SVMmodel),('dt', DTmodel), ('lr', LRmodel), ('knn', Kmodel)], voting='soft')

# Train the ensemble model
ensemble_model.fit(X_train5, y_train5)

# Make predictions on the testing set
y_pred = ensemble_model.predict(x_test5)

# Calculate the accuracy of the ensemble model
accuracy = accuracy_score(y_test5, y_pred)
print("Accuracy:", accuracy)

"""ENSEMBLE hard voting"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import VotingClassifier
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier



SVMmodel = SVC(probability=True)


# Create the ensemble model using VotingClassifier
ensemble_model = VotingClassifier(estimators=[('svm', SVMmodel),('dt', DTmodel), ('lr', LRmodel), ('knn', Kmodel)], voting='hard')

# Train the ensemble model
ensemble_model.fit(X_train5, y_train5)

# Make predictions on the testing set
y_pred = ensemble_model.predict(x_test5)

# Calculate the accuracy of the ensemble model
accuracy = accuracy_score(y_test5, y_pred)
print("Accuracy:", accuracy)

"""Dataset 3"""

import pandas as pd

csvFile2 = pd.read_csv('/content/drive/MyDrive/dataset csv/spanish.csv')
metadata2 =pd.DataFrame(csvFile2)

metadata2

X5 = metadata2.iloc[:,:-1]
y5 = metadata2.gender

X_train6,x_test6,y_train6,y_test6 = train_test_split(X5,y5,test_size=0.2,random_state = 1)

from sklearn.model_selection import train_test_split

"""SVM (dataset3)"""

def score(model):
    print("Training score: ",model.score(X_train6,y_train6))
    print("Test score: ",model.score(x_test6,y_test6))

from sklearn.svm import SVC

SVMmodel = SVC(kernel = 'rbf', C=2.0,random_state=0,degree = 3)
SVMmodel.fit(X_train6,y_train6)
ypred6 = SVMmodel.predict(x_test6)
score(SVMmodel)

"""DT"""

from sklearn.tree import DecisionTreeClassifier

DTmodel = DecisionTreeClassifier(min_samples_split = 5,max_depth = 10,random_state = 0)

DTmodel.fit(X_train6,y_train6)

ypred7 = DTmodel.predict(x_test6)

score(DTmodel)

"""LR"""

from sklearn.linear_model import LogisticRegression
LRmodel = LogisticRegression(n_jobs=3,max_iter=1000,random_state=0)

LRmodel.fit(X_train6,y_train6)
ypred7 = LRmodel.predict(x_test6)
score(LRmodel)

"""KNN"""

from sklearn.neighbors import KNeighborsClassifier
Kmodel = KNeighborsClassifier(n_neighbors = 50,metric ='minkowski',p=1,n_jobs=5,algorithm='ball_tree')
Kmodel.fit(X_train6,y_train6)
ypred9 = Kmodel.predict(x_test6)
score(Kmodel)

"""ENSEMBLE SOFT VOTING"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import VotingClassifier
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier



SVMmodel = SVC(probability=True)


# Create the ensemble model using VotingClassifier
ensemble_model = VotingClassifier(estimators=[('svm', SVMmodel),('dt', DTmodel), ('lr', LRmodel), ('knn', Kmodel)], voting='soft')

# Train the ensemble model
ensemble_model.fit(X_train6, y_train6)

# Make predictions on the testing set
y_pred = ensemble_model.predict(x_test6)

# Calculate the accuracy of the ensemble model
accuracy = accuracy_score(y_test6, y_pred)
print("Accuracy:", accuracy)

"""ENSEMBLE HARD VOTING"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import VotingClassifier
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier



SVMmodel = SVC(probability=True)


# Create the ensemble model using VotingClassifier
ensemble_model = VotingClassifier(estimators=[('svm', SVMmodel),('dt', DTmodel), ('lr', LRmodel), ('knn', Kmodel)], voting='soft')

# Train the ensemble model
ensemble_model.fit(X_train6, y_train6)

# Make predictions on the testing set
y_pred = ensemble_model.predict(x_test6)

# Calculate the accuracy of the ensemble model
accuracy = accuracy_score(y_test6, y_pred)
print("Accuracy:", accuracy)

"""mean, median

"""

import scipy
_path = "/content/drive/MyDrive/dataset1"

import glob
wavefiles=glob.glob(_path+"/*.wav")
# print(wavefiles)

meanlist=[]

stdlist=[]
maxvlist=[]
type_list= []
n_list= []
minvlist = []
medianlist =[]
modelist = []
skew_list=[]
kurt_list=[]
q1_list =[]
q3_list=[]
iqr_list = []

# n_list = []
# j_list = []
# s_list = []
# h_list = []

for wav_file in  wavefiles :
  sound = parselmouth.Sound(wav_file) # sound object from wav file
  pitch = sound.to_pitch()
  pulses = parselmouth.praat.call([sound, pitch], "To PointProcess (cc)")

        # name analysis
  name = os.path.basename(wav_file).split(".")[0]

       # jitter
  # jitter_local = parselmouth.praat.call(pulses, "Get jitter (local)", 0.0, 0.0, 0.0001, 0.02, 1.3) * 100

  #       # shimmer
  # shimmer_local = parselmouth.praat.call([sound, pulses], "Get shimmer (local)", 0, 0, 0.0001, 0.02, 1.3, 1.6)

  #       # HNR
  # harmonicity = parselmouth.praat.call(sound, "To Harmonicity (cc)", 0.01, 75, 0.1, 1.0)
  # hnr = parselmouth.praat.call(harmonicity, "Get mean", 0, 0)

        # Append to numpy array
  # n_list.append(name)
  # j_list.append(jitter_local)
  # s_list.append(shimmer_local)
  # h_list.append(hnr)

  # if jitter_local >2.10:
  #   type_list.append("1")
  # else:
  #   type_list.append("0")

  x, sr = librosa.load(wav_file)
  freqs = np.fft.fftfreq(x.size)






  mean = 0
  std = 0
  maxv = 0
  minv = 0
  median = 0
  mode = 0
  skew = 0
  kurt = 0
  q1 = 0
  q3 = 0
  iqr = 0


  mean = np.mean(freqs)
  std = np.std(freqs)
  maxv = np.amax(freqs)
  minv = np.amin(freqs)
  median = np.median(freqs)
  mode = scipy.stats.mode(freqs)[0][0]
  skew = scipy.stats.skew(freqs)
  kurt = scipy.stats.kurtosis(freqs)
  q1 = np.quantile(freqs, 0.25)
  q3 = np.quantile(freqs, 0.75)
  iqr = scipy.stats.iqr(freqs)

  meanlist.append(mean)
  stdlist.append(std)
  maxvlist.append(maxv)
  minvlist.append(minv)
  medianlist.append(median)
  modelist.append(mode)
  skew_list.append(skew)
  kurt_list.append(kurt)
  q1_list.append(q1)
  q3_list.append(q3)

  iqr_list.append(iqr)

import pandas as pd
def get_voice_data(_path):




  return meanlist,stdlist,maxvlist,minvlist,medianlist, modelist, skew_list, kurt_list,q1_list,iqr_list

meanlist,stdlist,maxvlist,minvlist,medianlist,modelist, skew_list, kurt_list,q1_list,iqr_list   = get_voice_data(_path)

cols=["meanlist","stdlist","maxvlist","minvlist","medianlist","modelist", "skew_list", "kurt_list","q1_list","iqr_list" ]

mean=pd.DataFrame(meanlist, columns = ['Mean'])
std=pd.DataFrame(stdlist, columns = ['Std'])
max=pd.DataFrame(maxvlist, columns = ['Max'])
min=pd.DataFrame(minvlist, columns = ['Min'])
median=pd.DataFrame(medianlist, columns = ['Median'])
mode_ = pd.DataFrame(modelist, columns = ['Mode'])
skew_ = pd.DataFrame(skew_list, columns = ['Skew'])
kurt_ = pd.DataFrame(kurt_list, columns = ['Kurt'])
q1_ = pd.DataFrame(q1_list, columns = ['Q1'])
iqr_ = pd.DataFrame(iqr_list, columns = ['IQR'])


Feature = pd.concat([mean, std,max,min,median, mode_,skew_,kurt_, q1_,iqr_, df2], axis = 1)

# Feature1 = pd.concat([Feature, df2])

"""mean with rnn model"""

from sklearn.preprocessing import LabelEncoder

# Extract features and labels
features = Feature.iloc[:,:-1]
labels = Feature.label_list

# Convert labels to integers using LabelEncoder
label_encoder = LabelEncoder()
labels = label_encoder.fit_transform(labels)

# Split the data into training and testing sets
x_train12, x_test12, y_train12, y_test12 = train_test_split(features, labels, test_size=0.4, random_state=42)

# Normalize the features to range [0, 1]
x_train12 = x_train12.astype('float32') / np.max(features)
x_test12 = x_test12.astype('float32') / np.max(features)

x_train12 = x_train12.values
x_test12 = x_test12.values

x_train12 = x_train12.reshape(x_train12.shape[0], x_train12.shape[1], 1)
x_test12 = x_test12.reshape(x_test12.shape[0], x_test12.shape[1], 1)

num_classes = len(np.unique(labels))
y_train12 = keras.utils.to_categorical(y_train12, num_classes)
y_test12 = keras.utils.to_categorical(y_test12, num_classes)

model12 = keras.Sequential([
    layers.LSTM(64, input_shape=(x_train12.shape[1], 1), return_sequences=True),
    layers.LSTM(64),
    layers.Dense(128, activation='relu'),
    layers.Dense(num_classes, activation='softmax')
])

model12.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
batch_size = 16
epochs = 20

model12.fit(x_train12, y_train12, batch_size=batch_size, epochs=epochs, validation_split=0.1)

loss, accuracy = model11.evaluate(x_test11, y_test11)
print('Test accuracy:', accuracy)

"""mean with sequential model"""

X22= Feature.iloc[:,:-1]
y22= Feature.label_list
x_train9, x_test9, y_train9,  y_test9 = train_test_split(X22,y22, test_size=0.2, random_state=42)
model2 = Sequential()
model2.add(Dense(64, activation='relu', input_dim=x_train9.shape[1]))
model2.add(Dense(64, activation='relu'))
model2.add(Dense(1, activation='sigmoid'))

from sklearn.preprocessing import LabelEncoder

# Instantiate the LabelEncoder
label_encoder = LabelEncoder()

# Convert string labels to numeric values
y_train9 = label_encoder.fit_transform(y_train9)

model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

model2.fit(x_train9, y_train9, epochs=10, batch_size=32)

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
y_test_encoded = label_encoder.fit_transform(y_test9)

loss, accuracy = model2.evaluate(x_test9, y_test_encoded)
print("Test Loss:", loss)
print("Test Accuracy:", accuracy)

"""combination of mean and chroma"""

#chroma
scaled_feature1

Feature2

comb =Feature = pd.concat([new1,Feature2], axis = 1)

comb

"""combination of mean and chroma with RNN model"""

# Extract features and labels
features = comb.iloc[:,:-1]
labels = comb.label_list

# Convert labels to integers using LabelEncoder
label_encoder = LabelEncoder()
labels = label_encoder.fit_transform(labels)

# Split the data into training and testing sets
x_train13, x_test13, y_train13, y_test13 = train_test_split(features, labels, test_size=0.2, random_state=42)

# Normalize the features to range [0, 1]
x_train13 = x_train13.astype('float32') / np.max(features)
x_test13 = x_test13.astype('float32') / np.max(features)

x_train13 = x_train13.values
x_test13 = x_test13.values

x_train13 = x_train13.reshape(x_train13.shape[0], x_train13.shape[1], 1)
x_test13 = x_test13.reshape(x_test13.shape[0], x_test13.shape[1], 1)

num_classes = len(np.unique(labels))
y_train13 = keras.utils.to_categorical(y_train13, num_classes)
y_test13 = keras.utils.to_categorical(y_test13, num_classes)

model13 = keras.Sequential([
    layers.LSTM(64, input_shape=(x_train13.shape[1], 1), return_sequences=True),
    layers.LSTM(64),
    layers.Dense(128, activation='relu'),
    layers.Dense(num_classes, activation='softmax')
])

model13.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
batch_size = 16
epochs = 20

model13.fit(x_train13, y_train13, batch_size=batch_size, epochs=epochs, validation_split=0.1)

loss, accuracy = model13.evaluate(x_test13, y_test13)
print('Test accuracy:', accuracy)

"""combination of mean and chroma with sequential model"""

X23= comb.iloc[:,:-1]
y23= comb.label_list
x_train9, x_test9, y_train9,  y_test9 = train_test_split(X23,y23, test_size=0.2, random_state=42)
model2 = Sequential()
model2.add(Dense(64, activation='relu', input_dim=x_train9.shape[1]))
model2.add(Dense(64, activation='relu'))
model2.add(Dense(1, activation='sigmoid'))

from sklearn.preprocessing import LabelEncoder

# Instantiate the LabelEncoder
label_encoder = LabelEncoder()

# Convert string labels to numeric values
y_train9 = label_encoder.fit_transform(y_train9)

model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

model2.fit(x_train9, y_train9, epochs=10, batch_size=32)

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
y_test_encoded = label_encoder.fit_transform(y_test9)

loss, accuracy = model2.evaluate(x_test9, y_test_encoded)
print("Test Loss:", loss)
print("Test Accuracy:", accuracy)